{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder paths\n",
    "half_hourly_folder = 'Input/hhblock_dataset/hhblock_dataset/'\n",
    "daily_dataset_folder = 'Input/daily_dataset/daily_dataset/'\n",
    "new_half_hourly_folder = 'new_half_hourly'  # New folder to save merged files\n",
    "\n",
    "# Create the new folder if it doesn't exist\n",
    "if not os.path.exists(new_half_hourly_folder):\n",
    "    os.makedirs(new_half_hourly_folder)\n",
    "\n",
    "# Function to process and merge data for one file\n",
    "def process_and_merge(file_name, block_value=None):\n",
    "    # Load half-hourly and daily data\n",
    "    half_hourly_data = pd.read_csv(os.path.join(half_hourly_folder, file_name))\n",
    "    daily_data = pd.read_csv(os.path.join(daily_dataset_folder, file_name))\n",
    "\n",
    "    # Convert 'day' column to datetime objects\n",
    "    half_hourly_data['day'] = pd.to_datetime(half_hourly_data['day']).dt.date\n",
    "    daily_data['day'] = pd.to_datetime(daily_data['day']).dt.date\n",
    "\n",
    "    # Merge the dataframes based on 'LCLid' and 'day'\n",
    "    merged_data = pd.merge(half_hourly_data, daily_data, on=['LCLid', 'day'], how='left')\n",
    "\n",
    "    # Reorder columns and fill missing energy-related columns with zeros\n",
    "    energy_cols = ['energy_median', 'energy_mean', 'energy_max', 'energy_count', 'energy_std', 'energy_sum', 'energy_min']\n",
    "    merged_data[energy_cols] = merged_data[energy_cols].fillna(0)\n",
    "\n",
    "    # If block_value is provided, add a 'block' column with the specified value\n",
    "    if block_value:\n",
    "        merged_data['block'] = block_value\n",
    "\n",
    "    # Save the merged data to the 'new_half_hourly' folder\n",
    "    output_file_name = os.path.join(new_half_hourly_folder, 'merged_' + file_name)\n",
    "    merged_data.to_csv(output_file_name, index=False)\n",
    "\n",
    "# Process all files in the 'half_hourly' folder\n",
    "for file_name in os.listdir(half_hourly_folder):\n",
    "    # Extract the block value from the filename (e.g., block_2.csv)\n",
    "    block_value = file_name.split('_')[1].split('.')[0] if 'block_' in file_name else None\n",
    "\n",
    "    process_and_merge(file_name, block_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the 'information_households.csv' file\n",
    "information_file = 'Input/informations_households.csv'\n",
    "information_data = pd.read_csv(information_file)\n",
    "new_half_hourly_folder = 'new_half_hourly'  # New folder to save merged files\n",
    "\n",
    "# Iterate through each unique 'file' value in the 'information_households.csv' file\n",
    "for block_file in information_data['file'].unique():\n",
    "    # Filter rows based on the 'file' value\n",
    "    block_data = information_data[information_data['file'] == block_file]\n",
    "\n",
    "    # Read the corresponding 'merged_block_X.csv' file from the \"new_half_hourly\" folder\n",
    "    merged_block_file = f'merged_{block_file}.csv'\n",
    "    merged_block_data = pd.read_csv(os.path.join(new_half_hourly_folder, merged_block_file))\n",
    "\n",
    "    # Merge the data based on the 'LCLid'\n",
    "    merged_data = pd.merge(merged_block_data, block_data[['LCLid', 'stdorToU', 'Acorn', 'Acorn_grouped']],\n",
    "                           on='LCLid', how='left')\n",
    "\n",
    "    # Fill missing values in the 'merged_block_X.csv' file with values from 'information_households.csv'\n",
    "    for column in ['stdorToU', 'Acorn', 'Acorn_grouped']:\n",
    "        if f'{column}_x' in merged_data.columns and f'{column}_y' in merged_data.columns:\n",
    "            merged_data[column] = merged_data[f'{column}_x'].combine_first(merged_data[f'{column}_y'])\n",
    "        elif f'{column}_x' in merged_data.columns:\n",
    "            merged_data[column] = merged_data[f'{column}_x']\n",
    "        elif f'{column}_y' in merged_data.columns:\n",
    "            merged_data[column] = merged_data[f'{column}_y']\n",
    "\n",
    "    # Drop the intermediate columns, if they exist\n",
    "    columns_to_drop = [f'{column}_x' for column in ['stdorToU', 'Acorn', 'Acorn_grouped']] + \\\n",
    "                      [f'{column}_y' for column in ['stdorToU', 'Acorn', 'Acorn_grouped']]\n",
    "    merged_data = merged_data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # Save the updated 'merged_block_X.csv' file in the \"new_half_hourly\" folder\n",
    "    merged_data.to_csv(os.path.join(new_half_hourly_folder, merged_block_file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your DataFrame from the 'weather_daily.csv' file\n",
    "weather_data = pd.read_csv('Input/weather_daily_darksky.csv')\n",
    "\n",
    "# Convert the 'time' column to datetime\n",
    "weather_data['time'] = pd.to_datetime(weather_data['time'])\n",
    "\n",
    "# Extract the date in the format 'yyyy-mm-dd' and assign it to the 'day' column\n",
    "weather_data['day'] = weather_data['time'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Reorder the columns with 'day' as the first column\n",
    "weather_data = weather_data[['day'] + [col for col in weather_data.columns if col != 'day']]\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "weather_data.to_csv('weather_daily_with_day.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder paths\n",
    "new_half_hourly_folder = 'new_half_hourly'  # Folder with 'new_half_hourly' files\n",
    "output_folder = 'new_half_hourly_with_day'  # Folder to save merged files\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load the weather data\n",
    "weather_data = pd.read_csv('weather_daily_with_day.csv')\n",
    "\n",
    "# List the files in the 'new_half_hourly' folder\n",
    "files = os.listdir(new_half_hourly_folder)\n",
    "\n",
    "# Iterate through each file in the 'new_half_hourly' folder\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):\n",
    "        # Read the 'new_half_hourly' file\n",
    "        half_hourly_data = pd.read_csv(os.path.join(new_half_hourly_folder, file))\n",
    "\n",
    "        # Merge the data based on the 'day' column\n",
    "        merged_data = pd.merge(half_hourly_data, weather_data, on='day', how='left')\n",
    "\n",
    "        # Save the updated 'new_half_hourly' file to the output folder\n",
    "        output_file = os.path.join(output_folder, file)\n",
    "        merged_data.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Source directory with CSV files\n",
    "source_dir = \"new_half_hourly_with_day\"\n",
    "\n",
    "# Destination directory for preprocessed CSV files\n",
    "dest_dir = \"preprocessing\"\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through files in the source directory\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(source_dir, filename)\n",
    "        dest_file_path = os.path.join(dest_dir, filename)\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Select the columns you need\n",
    "        selected_columns = ['LCLid', 'day'] + [f'hh_{i}' for i in range(48)]\n",
    "        df = df[selected_columns]\n",
    "\n",
    "        # Save the preprocessed data to the destination directory\n",
    "        df.to_csv(dest_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create a folder to store individual LCLid files\n",
    "output_folder = \"LCLid_files\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# List all files in the \"preprocessing\" folder\n",
    "preprocessing_folder = \"preprocessing\"\n",
    "all_files = os.listdir(preprocessing_folder)\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to convert \"hh\" format to time intervals\n",
    "def convert_hh_format(hh):\n",
    "    hours, minutes = divmod(int(hh.split(\"_\")[1]) * 30, 60)\n",
    "    return f\"{hours:02}:{minutes:02}:00\"\n",
    "\n",
    "# Process each file\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        # Read the file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(preprocessing_folder, file))\n",
    "        \n",
    "        # Group the data by LCLid\n",
    "        grouped = df.groupby(\"LCLid\")\n",
    "        \n",
    "        # Process each unique LCLid\n",
    "        for name, group_data in grouped:\n",
    "            # Extract relevant columns\n",
    "            date_col = group_data[[\"day\"]]\n",
    "            consumption_col = group_data.iloc[:, 2:50]\n",
    "            \n",
    "            # Create a DataFrame with the desired format\n",
    "            date_time_consumption = pd.concat([date_col, consumption_col], axis=1)\n",
    "            \n",
    "            # Reshape the DataFrame to long format\n",
    "            date_time_consumption = date_time_consumption.melt(id_vars=[\"day\"], var_name=\"time\", value_name=\"consumption\")\n",
    "            \n",
    "            # Convert \"time\" column to time intervals\n",
    "            date_time_consumption[\"time\"] = date_time_consumption[\"time\"].apply(convert_hh_format)\n",
    "            \n",
    "            # Create a \"datetime\" column by combining \"day\" and \"time\"\n",
    "            date_time_consumption[\"datetime\"] = pd.to_datetime(date_time_consumption[\"day\"] + \" \" + date_time_consumption[\"time\"])\n",
    "            \n",
    "            # Sort by datetime\n",
    "            date_time_consumption = date_time_consumption.sort_values(by=\"datetime\")\n",
    "            \n",
    "            # Select only the \"datetime\" and \"consumption\" columns\n",
    "            date_time_consumption = date_time_consumption[[\"datetime\", \"consumption\"]]\n",
    "            \n",
    "            # Save the DataFrame as a CSV file named by LCLid\n",
    "            output_file = os.path.join(output_folder, name + \".csv\")\n",
    "            \n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(output_file):\n",
    "                # Append to the existing file\n",
    "                date_time_consumption.to_csv(output_file, index=False, mode='a', header=False)\n",
    "            else:\n",
    "                # Create a new file if it doesn't exist\n",
    "                date_time_consumption.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Read the original weather data file\n",
    "original_weather_data = pd.read_csv(\"Input/weather_hourly_darksky.csv\")\n",
    "\n",
    "# Select only the \"temperature\" and \"time\" fields\n",
    "weather_data = original_weather_data[[\"time\", \"temperature\"]]\n",
    "\n",
    "# Convert the \"time\" field to datetime\n",
    "weather_data['time'] = pd.to_datetime(weather_data['time'])\n",
    "\n",
    "# Sort the DataFrame by time in ascending order\n",
    "weather_data = weather_data.sort_values(by='time')\n",
    "\n",
    "# Create a new DataFrame for half-hourly data\n",
    "half_hourly_weather_data = pd.DataFrame()\n",
    "\n",
    "# Initialize a variable to count the records processed\n",
    "records_processed = 0\n",
    "\n",
    "# Initialize index\n",
    "index = 0\n",
    "\n",
    "# Iterate through the sorted weather data\n",
    "while index < len(weather_data):\n",
    "    row = weather_data.iloc[index]\n",
    "    half_hourly_weather_data = pd.concat([half_hourly_weather_data, pd.DataFrame([row])])\n",
    "\n",
    "    next_time = row['time'] + timedelta(minutes=30)\n",
    "    if index < len(weather_data) - 1 and next_time < weather_data.iloc[index + 1]['time']:\n",
    "        interpolated_row = row.copy()\n",
    "        interpolated_row['time'] = next_time\n",
    "        half_hourly_weather_data = pd.concat([half_hourly_weather_data, pd.DataFrame([interpolated_row])])\n",
    "        records_processed += 1\n",
    "        print(f\"Records Processed: {records_processed} / Total Records: {len(weather_data)}\")\n",
    "    index += 1\n",
    "\n",
    "# Sort the DataFrame by time\n",
    "half_hourly_weather_data = half_hourly_weather_data.sort_values(by='time')\n",
    "\n",
    "# Forward-fill temperature values for missing half-hourly records\n",
    "half_hourly_weather_data['temperature'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Save the half-hourly weather data to a new CSV file\n",
    "half_hourly_weather_data.to_csv(\"half_hourly_weather.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create a folder to store the output files\n",
    "if not os.path.exists(\"LCLid_files_with_temp\"):\n",
    "    os.makedirs(\"LCLid_files_with_temp\")\n",
    "\n",
    "# Load the half-hourly weather data\n",
    "weather_data = pd.read_csv(\"half_hourly_weather.csv\")\n",
    "\n",
    "# Get the list of files in the LCLid_files folder\n",
    "input_folder = \"LCLid_files\"\n",
    "all_files = os.listdir(input_folder)\n",
    "\n",
    "# Iterate through each file\n",
    "for file in all_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        # Read the LCLid file\n",
    "        df = pd.read_csv(os.path.join(input_folder, file))\n",
    "        \n",
    "        # Merge the LCLid data with the weather data based on both 'datetime' and 'time' columns\n",
    "        merged_df = pd.merge(df, weather_data, left_on=\"datetime\", right_on=\"time\", how=\"left\")\n",
    "        \n",
    "        # Create a new folder for output files if it doesn't exist\n",
    "        output_folder = \"LCLid_files_with_temp\"\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        \n",
    "        # Save the merged data to a new CSV file\n",
    "        output_file = os.path.join(output_folder, file)\n",
    "        merged_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "data_dir = \"LcLid_files_with_temp\"\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through the files in the directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Extract the household ID from the filename\n",
    "        household = filename.split(\".\")[0]\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(data_dir, filename), usecols=[\"datetime\", \"consumption\", \"temperature\"])\n",
    "\n",
    "        # Add the \"household\" column with the corresponding household ID\n",
    "        df[\"household\"] = household\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Sort the combined DataFrame by \"datetime\" and \"household\"\n",
    "combined_data.sort_values(by=[\"datetime\", \"household\"], inplace=True)\n",
    "\n",
    "# Save the combined data to a single CSV file\n",
    "combined_data.to_csv(\"combined_data.csv\", index=False)\n",
    "\n",
    "# The combined data is now saved to \"combined_data.csv\" in the current working directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the process for training a Simple RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "epoch = 50\n",
    "# Function to create and train a S\n",
    "# imple RNN model\n",
    "def train_rnn_model(X_train, y_train):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, activation='relu', input_shape=(None, 1)))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = Adam(clipvalue=0.5)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    history = model.fit(X_train, y_train, epochs=epoch, batch_size=32, verbose=0)\n",
    "    return model\n",
    "\n",
    "# Function to predict and return results\n",
    "def predict(model, X_test):\n",
    "    predicted_values = model.predict(X_test)\n",
    "    return predicted_values\n",
    "\n",
    "# Load data from combined_data.csv\n",
    "percentage_to_load = 100\n",
    "data = pd.read_csv(\"combined_data.csv\", usecols=['datetime', 'consumption', 'temperature', 'household'], nrows=int(percentage_to_load / 100 * len(pd.read_csv(\"combined_data.csv\"))))\n",
    "\n",
    "# Sort the data by datetime\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data.sort_values(by='datetime', inplace=True)\n",
    "\n",
    "# Split the data into training (70%) and testing (30%)\n",
    "unique_datetimes = data['datetime'].unique()\n",
    "train_size = int(0.7 * len(unique_datetimes))\n",
    "train_datetimes = unique_datetimes[:train_size]\n",
    "test_datetimes = unique_datetimes[train_size:]\n",
    "train_data = data[data['datetime'].isin(train_datetimes)]\n",
    "test_data = data[data['datetime'].isin(test_datetimes)]\n",
    "\n",
    "# Calculate the mean of the 'consumption' column\n",
    "mean_consumption = train_data['consumption'].mean()\n",
    "train_data['consumption'].fillna(mean_consumption, inplace=True)\n",
    "test_data['consumption'].fillna(mean_consumption, inplace=True)\n",
    "\n",
    "# Prepare the training data\n",
    "X_train = train_data['temperature'].values\n",
    "y_train = train_data['consumption'].values\n",
    "# Check the dimensions of X_train and y_train\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "X_train = X_train.reshape(-1, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "# Create and train a Simple RNN model\n",
    "\n",
    "start_time = time.time()\n",
    "# Perform model training here\n",
    "model = train_rnn_model(X_train, y_train)\n",
    "print(\"Finished training\")\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "history = model.history.history\n",
    "\n",
    "# Now you can use or save the training history as needed\n",
    "training_loss = history['loss']\n",
    "validation_loss = history.get('val_loss', None)\n",
    "\n",
    "print(training_loss)\n",
    "print(validation_loss)\n",
    "# Prepare the testing data\n",
    "X_test = test_data['temperature'].values\n",
    "y_test = test_data['consumption'].values\n",
    "# Check the dimensions of X_train and y_train\n",
    "print(f\"X_train shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_test.shape}\")\n",
    "\n",
    "X_test = X_test.reshape(-1, 1, 1)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "# Perform model testing here\n",
    "# Make predictions using the model\n",
    "predicted_values = predict(model, X_test)\n",
    "end_time = time.time()\n",
    "testing_time = end_time - start_time\n",
    "print(f\"Testing time: {testing_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Check for NaN values in the NumPy array\n",
    "nan_indices = np.isnan(predicted_values)\n",
    "mean_value = np.nanmean(predicted_values)\n",
    "predicted_values[nan_indices] = mean_value\n",
    "print(\"Finished testing\")\n",
    "\n",
    "print('Calculate metrics for the testing data') \n",
    "mse = mean_squared_error(y_test, predicted_values)\n",
    "mae = mean_absolute_error(y_test, predicted_values)\n",
    "r2 = r2_score(y_test, predicted_values)\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = 100 * (mae / np.mean(y_test))\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
    "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n",
    "print(f'R-squared (R2): {r2:.2f}')\n",
    "print('Mean Absolute Percentage Error (MAPE): {:.2f}%'.format(mape))\n",
    "print(\"this is individual\")\n",
    "\n",
    "# Create a list of metrics and their values\n",
    "metrics_data = [\n",
    "    ['MSE', mse],\n",
    "    ['MAE', mae],\n",
    "    ['R2', r2],\n",
    "    ['MAPE', mape],\n",
    "    ['Training Time', training_time],\n",
    "    ['Testing Time', testing_time],\n",
    "    ['Epochs', epoch]\n",
    "]\n",
    "\n",
    "# Create the \"metrics\" folder if it doesn't exist\n",
    "metrics_folder = 'metrics'\n",
    "if not os.path.exists(metrics_folder):\n",
    "    os.makedirs(metrics_folder)\n",
    "\n",
    "# Set the path to save the table file\n",
    "table_file_path = os.path.join(metrics_folder, 'RNN_daily_individual_metrics.txt')\n",
    "\n",
    "# Print metrics in a table\n",
    "table = tabulate(metrics_data, headers=['Metric', 'Value'], tablefmt='grid')\n",
    "\n",
    "\n",
    "# Save the table to the specified file path\n",
    "with open(table_file_path, \"w\") as file:\n",
    "    file.write(table)\n",
    "# Calculate the number of days for the test data\n",
    "num_days = len(test_data['datetime'].dt.date.unique())\n",
    "\n",
    "# Create a DataFrame for predicted data with the same datetime values\n",
    "predicted_data = pd.DataFrame({'datetime': test_data['datetime'], 'consumption': predicted_values[:, 0]})\n",
    "\n",
    "# Create a DataFrame for test data\n",
    "test_data_df = pd.DataFrame({'datetime': test_data['datetime'], 'consumption': test_data['consumption']})\n",
    "\n",
    "# Aggregate daily test consumption data\n",
    "daily_test_data = test_data_df.groupby(test_data_df['datetime'].dt.date)['consumption'].sum()\n",
    "\n",
    "# Aggregate daily predicted consumption data\n",
    "daily_predicted_data = predicted_data.groupby(predicted_data['datetime'].dt.date)['consumption'].sum()\n",
    "\n",
    "\n",
    "only_daily_test_data = daily_test_data.values\n",
    "only_predicted_data = daily_predicted_data.values\n",
    "\n",
    "\n",
    "print('Calculate metrics for the testing data') \n",
    "mse = mean_squared_error(only_daily_test_data, only_predicted_data)\n",
    "mae = mean_absolute_error(only_daily_test_data, only_predicted_data)\n",
    "r2 = r2_score(only_daily_test_data, only_predicted_data)\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = 100 * (mae / np.mean(only_daily_test_data))\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
    "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n",
    "print(f'R-squared (R2): {r2:.2f}')\n",
    "print('Mean Absolute Percentage Error (MAPE): {:.2f}%'.format(mape))\n",
    "\n",
    "# Create a list of metrics and their values\n",
    "aggregate_metrics_data = [\n",
    "    ['MSE', mse],\n",
    "    ['MAE', mae],\n",
    "    ['R2', r2],\n",
    "    ['MAPE', mape],\n",
    "    ['Training Time', training_time],\n",
    "    ['Testing Time', testing_time],\n",
    "    ['Epochs', epoch]\n",
    "]\n",
    "\n",
    "# Create the \"metrics\" folder if it doesn't exist\n",
    "metrics_folder = 'metrics'\n",
    "if not os.path.exists(metrics_folder):\n",
    "    os.makedirs(metrics_folder)\n",
    "\n",
    "# Set the path to save the table file\n",
    "table_file_path = os.path.join(metrics_folder, 'RNN_daily_aggregate_metrics.txt')\n",
    "\n",
    "# Print metrics in a table\n",
    "table = tabulate(aggregate_metrics_data, headers=['Metric', 'Value'], tablefmt='grid')\n",
    "\n",
    "\n",
    "# Save the table to the specified file path\n",
    "with open(table_file_path, \"w\") as file:\n",
    "    file.write(table)\n",
    "# Get the unique dates in the data\n",
    "#unique_dates = daily_test_data.index\n",
    "\n",
    "# Create a plot\n",
    "#plt.figure(figsize=(12, 6))\n",
    "#plt.plot(unique_dates, daily_test_data.values, marker='o', label='Actual Consumption', linestyle='-')\n",
    "#plt.plot(unique_dates, daily_predicted_data.values, marker='o', label='Predicted Consumption', linestyle='--')\n",
    "\n",
    "#plt.xlabel('Date')\n",
    "#plt.ylabel('Consumption')\n",
    "#plt.title('Actual vs. Predicted Consumption for Each Day')\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# Get the unique dates in the data\n",
    "unique_dates = daily_test_data.index\n",
    "value = 7\n",
    "# Iterate through the unique dates, creating a plot for each pair of dates\n",
    "for i in range(0, len(unique_dates), value):\n",
    "    if i + value-1 < len(unique_dates):\n",
    "        dateArray = []\n",
    "        for n in range(i,i+value):\n",
    "            date = unique_dates[n]\n",
    "            dateArray.append(date)\n",
    "\n",
    "        date1 = unique_dates[i]\n",
    "        date2 = unique_dates[i + 1]\n",
    "        # Get consumption values for the two dates\n",
    "        consumptionarray = []\n",
    "        for date in dateArray:\n",
    "            consumption = daily_test_data.loc[date]\n",
    "            consumptionarray.append(consumption)\n",
    "\n",
    "        # Get predicted consumption values for the two dates (adjust the data source accordingly)\n",
    "        # Get consumption values for the two dates\n",
    "        pconsumptionarray = []\n",
    "        for date in dateArray:\n",
    "            consumption = daily_predicted_data.loc[date]\n",
    "            pconsumptionarray.append(consumption)\n",
    "\n",
    "        # Create a plot with actual and predicted consumption\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(dateArray, consumptionarray, marker='o', label='Actual Consumption', linestyle='-')\n",
    "        plt.plot(dateArray, pconsumptionarray, marker='o', label='Predicted Consumption', linestyle='--')\n",
    "\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Consumption')\n",
    "        plt.title(f'Actual vs. Predicted Consumption between {dateArray[0]} and {dateArray[-1]}')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Set the x-axis ticks and format date labels\n",
    "        date_ticks = dateArray\n",
    "        date_labels = [date.strftime('%Y-%m-%d') for date in date_ticks]\n",
    "        plt.xticks(date_ticks, date_labels, rotation=45)\n",
    "        \n",
    "        # Save the plot to a folder named \"plots\"\n",
    "        if not os.path.exists(\"rnn_plots\"):\n",
    "            os.makedirs(\"rnn_plots\")\n",
    "        plt.savefig(f\"rnn_plots/RNN_{dateArray[0]}_to_{dateArray[-1]}.png\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Clear the current figure\n",
    "    plt.clf()\n",
    "\n",
    "    # Close the plot window\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is for training an LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
