{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import Image\n",
    "\n",
    "half_hourly_data = pd.read_csv('Input/halfhourly_dataset/halfhourly_dataset/block_0.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Rows: 1231776, Unique Days: 819\n",
      "Number of unique days in the dataset: 819\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load your half-hourly data\n",
    "half_hourly_data = pd.read_csv('Input/halfhourly_dataset/halfhourly_dataset/block_0.csv')\n",
    "\n",
    "# Convert 'tstp' column to datetime\n",
    "half_hourly_data['tstp'] = pd.to_datetime(half_hourly_data['tstp'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Sort the data by LCLid and tstp\n",
    "half_hourly_data.sort_values(by=['LCLid', 'tstp'], inplace=True)\n",
    "\n",
    "# Group the data by LCLid\n",
    "grouped = half_hourly_data.groupby('LCLid')\n",
    "\n",
    "# Define a function to fill missing records for each group\n",
    "def fill_missing(group):\n",
    "    # Create a date range for 48 records (assuming the data starts at midnight)\n",
    "    start_time = group['tstp'].min().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    end_time = group['tstp'].max()\n",
    "    date_range = pd.date_range(start=start_time, end=end_time, freq='30T')\n",
    "\n",
    "    # Check which records are missing\n",
    "    missing_records = date_range[~date_range.isin(group['tstp'])]\n",
    "\n",
    "    # Create empty records for missing times\n",
    "    missing_data = pd.DataFrame({'tstp': missing_records, 'LCLid': group['LCLid'].iloc[0], 'energy(kWh/hh)': 0.0})\n",
    "\n",
    "    # Concatenate the missing data with the existing data for the group\n",
    "    return pd.concat([group, missing_data])\n",
    "\n",
    "# Apply the fill_missing function to each group and concatenate the results\n",
    "filled_data = grouped.apply(fill_missing)\n",
    "\n",
    "# Reset the index of the resulting DataFrame\n",
    "filled_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Sort the filled data\n",
    "filled_data.sort_values(by=['LCLid', 'tstp'], inplace=True)\n",
    "\n",
    "# Save the filled data to a CSV file\n",
    "filled_data.to_csv('filled_half_hourly_data.csv', index=False)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'filled_half_hourly_data.csv' file\n",
    "data = pd.read_csv('filled_half_hourly_data.csv')\n",
    "\n",
    "# Count the number of unique rows\n",
    "unique_rows = data.drop_duplicates().shape[0]\n",
    "\n",
    "# If the CSV contains a 'tstp' column, convert it to datetime and count unique days\n",
    "if 'tstp' in data.columns:\n",
    "    data['tstp'] = pd.to_datetime(data['tstp'])\n",
    "    unique_days = data['tstp'].dt.date.nunique()\n",
    "    print(f'Unique Rows: {unique_rows}, Unique Days: {unique_days}')\n",
    "else:\n",
    "    print(f'Unique Rows: {unique_rows}')\n",
    "\n",
    "\n",
    "# Count the number of unique days\n",
    "unique_days = filled_half_hourly_data['tstp'].dt.date.nunique()\n",
    "\n",
    "print(f'Number of unique days in the dataset: {unique_days}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the 'half_hourly.csv' and 'daily_dataset.csv' files\n",
    "half_hourly_data = pd.read_csv('Input/hhblock_dataset/hhblock_dataset/block_0.csv')\n",
    "daily_data = pd.read_csv('Input/daily_dataset/daily_dataset/block_0.csv')\n",
    "\n",
    "\n",
    "# Convert 'day' column to datetime objects\n",
    "half_hourly_data['day'] = pd.to_datetime(half_hourly_data['day']).dt.date\n",
    "daily_data['day'] = pd.to_datetime(daily_data['day']).dt.date\n",
    "\n",
    "# Merge the dataframes based on 'LCLid' and 'day'\n",
    "merged_data = pd.merge(half_hourly_data, daily_data, on=['LCLid', 'day'], how='left')\n",
    "\n",
    "# Reorder columns and fill missing energy-related columns with zeros\n",
    "energy_cols = ['energy_median', 'energy_mean', 'energy_max', 'energy_count', 'energy_std', 'energy_sum', 'energy_min']\n",
    "merged_data[energy_cols] = merged_data[energy_cols].fillna(0)\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv('merged_half_hourly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder paths\n",
    "half_hourly_folder = 'Input/hhblock_dataset/hhblock_dataset/'\n",
    "daily_dataset_folder = 'Input/daily_dataset/daily_dataset/'\n",
    "new_half_hourly_folder = 'new_half_hourly'  # New folder to save merged files\n",
    "\n",
    "# Create the new folder if it doesn't exist\n",
    "if not os.path.exists(new_half_hourly_folder):\n",
    "    os.makedirs(new_half_hourly_folder)\n",
    "\n",
    "# Function to process and merge data for one file\n",
    "def process_and_merge(file_name, block_value=None):\n",
    "    # Load half-hourly and daily data\n",
    "    half_hourly_data = pd.read_csv(os.path.join(half_hourly_folder, file_name))\n",
    "    daily_data = pd.read_csv(os.path.join(daily_dataset_folder, file_name))\n",
    "\n",
    "    # Convert 'day' column to datetime objects\n",
    "    half_hourly_data['day'] = pd.to_datetime(half_hourly_data['day']).dt.date\n",
    "    daily_data['day'] = pd.to_datetime(daily_data['day']).dt.date\n",
    "\n",
    "    # Merge the dataframes based on 'LCLid' and 'day'\n",
    "    merged_data = pd.merge(half_hourly_data, daily_data, on=['LCLid', 'day'], how='left')\n",
    "\n",
    "    # Reorder columns and fill missing energy-related columns with zeros\n",
    "    energy_cols = ['energy_median', 'energy_mean', 'energy_max', 'energy_count', 'energy_std', 'energy_sum', 'energy_min']\n",
    "    merged_data[energy_cols] = merged_data[energy_cols].fillna(0)\n",
    "\n",
    "    # If block_value is provided, add a 'block' column with the specified value\n",
    "    if block_value:\n",
    "        merged_data['block'] = block_value\n",
    "\n",
    "    # Save the merged data to the 'new_half_hourly' folder\n",
    "    output_file_name = os.path.join(new_half_hourly_folder, 'merged_' + file_name)\n",
    "    merged_data.to_csv(output_file_name, index=False)\n",
    "\n",
    "# Process all files in the 'half_hourly' folder\n",
    "for file_name in os.listdir(half_hourly_folder):\n",
    "    # Extract the block value from the filename (e.g., block_2.csv)\n",
    "    block_value = file_name.split('_')[1].split('.')[0] if 'block_' in file_name else None\n",
    "\n",
    "    process_and_merge(file_name, block_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the 'information_households.csv' file\n",
    "information_file = 'Input/informations_households.csv'\n",
    "information_data = pd.read_csv(information_file)\n",
    "new_half_hourly_folder = 'new_half_hourly'  # New folder to save merged files\n",
    "\n",
    "# Iterate through each unique 'file' value in the 'information_households.csv' file\n",
    "for block_file in information_data['file'].unique():\n",
    "    # Filter rows based on the 'file' value\n",
    "    block_data = information_data[information_data['file'] == block_file]\n",
    "\n",
    "    # Read the corresponding 'merged_block_X.csv' file from the \"new_half_hourly\" folder\n",
    "    merged_block_file = f'merged_{block_file}.csv'\n",
    "    merged_block_data = pd.read_csv(os.path.join(new_half_hourly_folder, merged_block_file))\n",
    "\n",
    "    # Merge the data based on the 'LCLid'\n",
    "    merged_data = pd.merge(merged_block_data, block_data[['LCLid', 'stdorToU', 'Acorn', 'Acorn_grouped']],\n",
    "                           on='LCLid', how='left')\n",
    "\n",
    "    # Fill missing values in the 'merged_block_X.csv' file with values from 'information_households.csv'\n",
    "    for column in ['stdorToU', 'Acorn', 'Acorn_grouped']:\n",
    "        if f'{column}_x' in merged_data.columns and f'{column}_y' in merged_data.columns:\n",
    "            merged_data[column] = merged_data[f'{column}_x'].combine_first(merged_data[f'{column}_y'])\n",
    "        elif f'{column}_x' in merged_data.columns:\n",
    "            merged_data[column] = merged_data[f'{column}_x']\n",
    "        elif f'{column}_y' in merged_data.columns:\n",
    "            merged_data[column] = merged_data[f'{column}_y']\n",
    "\n",
    "    # Drop the intermediate columns, if they exist\n",
    "    columns_to_drop = [f'{column}_x' for column in ['stdorToU', 'Acorn', 'Acorn_grouped']] + \\\n",
    "                      [f'{column}_y' for column in ['stdorToU', 'Acorn', 'Acorn_grouped']]\n",
    "    merged_data = merged_data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # Save the updated 'merged_block_X.csv' file in the \"new_half_hourly\" folder\n",
    "    merged_data.to_csv(os.path.join(new_half_hourly_folder, merged_block_file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your DataFrame from the 'weather_daily.csv' file\n",
    "weather_data = pd.read_csv('Input/weather_daily_darksky.csv')\n",
    "\n",
    "# Convert the 'time' column to datetime\n",
    "weather_data['time'] = pd.to_datetime(weather_data['time'])\n",
    "\n",
    "# Extract the date in the format 'yyyy-mm-dd' and assign it to the 'day' column\n",
    "weather_data['day'] = weather_data['time'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Reorder the columns with 'day' as the first column\n",
    "weather_data = weather_data[['day'] + [col for col in weather_data.columns if col != 'day']]\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "weather_data.to_csv('weather_daily_with_day.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder paths\n",
    "new_half_hourly_folder = 'new_half_hourly'  # Folder with 'new_half_hourly' files\n",
    "output_folder = 'new_half_hourly_with_day'  # Folder to save merged files\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load the weather data\n",
    "weather_data = pd.read_csv('weather_daily_with_day.csv')\n",
    "\n",
    "# List the files in the 'new_half_hourly' folder\n",
    "files = os.listdir(new_half_hourly_folder)\n",
    "\n",
    "# Iterate through each file in the 'new_half_hourly' folder\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):\n",
    "        # Read the 'new_half_hourly' file\n",
    "        half_hourly_data = pd.read_csv(os.path.join(new_half_hourly_folder, file))\n",
    "\n",
    "        # Merge the data based on the 'day' column\n",
    "        merged_data = pd.merge(half_hourly_data, weather_data, on='day', how='left')\n",
    "\n",
    "        # Save the updated 'new_half_hourly' file to the output folder\n",
    "        output_file = os.path.join(output_folder, file)\n",
    "        merged_data.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/steveburgos/Desktop/CS6301/DataExploration.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/steveburgos/Desktop/CS6301/DataExploration.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/steveburgos/Desktop/CS6301/DataExploration.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/steveburgos/Desktop/CS6301/DataExploration.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "# Define your custom RNN model\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(MyRNN, self).__init()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.target[idx]\n",
    "        return x, y\n",
    "\n",
    "# Specify the folder path containing CSV files\n",
    "folder_path = 'new_half_hourly_with_day'\n",
    "\n",
    "# Initialize your RNN model\n",
    "input_size = 1  # Modify based on your data\n",
    "hidden_size = 64  # Modify as needed\n",
    "num_layers = 1  # Modify based on your architecture\n",
    "output_size = 1  # Modify based on your problem\n",
    "rnn = MyRNN(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        # Load your dataset (assumes 'X' contains features and 'y' contains target)\n",
    "        # You will need to adapt this to load your specific data\n",
    "        X = np.random.rand(100, 10, 1)  # Example random data\n",
    "        y = np.random.rand(100, 1)  # Example random target\n",
    "        dataset = CustomDataset(X, y)\n",
    "\n",
    "        # Initialize DataLoader\n",
    "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        num_epochs = 10\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, labels in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = rnn(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(f'File: {file_name}, Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# You can then use the trained model for prediction on each file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
